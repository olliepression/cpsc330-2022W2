{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ASSIGNMENT CONFIG\n",
    "requirements: requirements.txt\n",
    "solutions_pdf: false\n",
    "export_cell: false\n",
    "check_all_cell: false\n",
    "generate:\n",
    "    pdf: false\n",
    "    zips: false\n",
    "    show_hidden: true\n",
    "    show_stdout: true\n",
    "files:\n",
    "    - data\n",
    "    - img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 4: Logistic regression, hyperparameter optimization \n",
    "### Associated lectures: [Lectures 7, 8](https://ubc-cs.github.io/cpsc330/README.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "<hr>\n",
    "rubric={points:6}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330/blob/master/docs/homework_instructions.md). \n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb.\n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "- Note that we are using the autograder for some of the coding questions so that you can get instant feedback. When you submit your homework on Gradescope, it will run your code on an AWS server. It's possible that all your tests pass locally but some test cases fail when you submit your work on Gradescope. Wait for your submission to run there and examine the autograder results.  If it fails, you are either submitting a wrong file or doing something unexpected somewhere in the code. If you cannot figure it out, make use of office hours and tutorials. It's important that you start early on the assignments so that we help you with such issues. Also, remember that passing the tests is not the only goal. Make sure you are understanding what exactly you are doing in each question and why you are doing it. \n",
    "\n",
    "_Note: The assignments will get gradually more open-ended as we progress through the course. In many cases, there won't be a single correct solution. Sometimes you will have to make your own choices and your own decisions (for example, on what parameter values to use when they are not explicitly provided in the instructions). Use your own judgment in such cases and justify your choices, if necessary._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q1\n",
    "points: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implementing `DummyClassifier` \n",
    "<hr>\n",
    "rubric={points:25}\n",
    "\n",
    "In this course (unlike CPSC 340) you will generally **not** be asked to implement machine learning algorihtms (like logistic regression) from scratch. However, this exercise is an exception: you will implement the simplest possible classifier, `DummyClassifier`.\n",
    "\n",
    "As a reminder, `DummyClassifier` is meant as a baseline and is generally the worst possible \"model\" you could \"fit\" to a dataset. All it does is predict the most popular class in the training set. So if there are more 0s than 1s it predicts 0 every time, and if there are more 1s than 0s it predicts 1 every time. For `predict_proba` it looks at the frequencies in the training set, so if you have 30% 0's 70% 1's it predicts `[0.3 0.7]` every time. Thus, `fit` only looks at `y` (not `X`).\n",
    "\n",
    "Below you will find starter code for a class called `MyDummyClassifier`, which has methods `fit()`, `predict()`, `predict_proba()` and `score()`. Your task is to fill in those four functions. To get your started, I have given you a `return` statement in each case that returns the correct data type: \n",
    "- `fit` can return nothing, \n",
    "- `predict` returns an array whose size is the number of examples, \n",
    "- `predict_proba` returns an array whose size is the number of examples x 2, and \n",
    "- `score` returns a number.\n",
    "\n",
    "The next code block has some tests you can use to assess whether your code is working. \n",
    "\n",
    "I suggest starting with `fit` and `predict`, and making sure those are working before moving on to `predict_proba`. For `predict_proba`, you should return the frequency of each class in the training data, which is the behaviour of `DummyClassifier(strategy='prior')`. Your `score` function should call your `predict` function. Again, you can compare with `DummyClassifier` using the code below.\n",
    "\n",
    "To simplify this question, you can assume **binary classification**, and furthermore that these classes are **encoded as 0 and 1**. In other words, you can assume that `y` contains only 0s and 1s. The real `DummyClassifier` works when you have more than two classes, and also works if the target values are encoded differently, for example as \"cat\", \"dog\", \"mouse\", etc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyDummyClassifier:\n",
    "    \"\"\"\n",
    "    A baseline classifier that predicts the most common class.\n",
    "    The predicted probabilities come from the relative frequencies\n",
    "    of the classes in the training data.\n",
    "\n",
    "    This implementation only works when y only contains 0s and 1s.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # BEGIN SOLUTION\n",
    "        self.prob_1 = np.mean(y == 1)\n",
    "        # END SOLUTION\n",
    "        return None  # Replace with your code\n",
    "\n",
    "    def predict(self, X):\n",
    "        # BEGIN SOLUTION\n",
    "        if self.prob_1 >= 0.5:\n",
    "            return np.ones(X.shape[0])\n",
    "        else:\n",
    "            return np.zeros(X.shape[0])\n",
    "        # END SOLUTION\n",
    "        return np.zeros(X.shape[0])  # Replace with your code\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # BEGIN SOLUTION\n",
    "        probs = np.zeros((X.shape[0], 2))\n",
    "        probs[:, 0] = 1 - self.prob_1\n",
    "        probs[:, 1] = self.prob_1\n",
    "        return probs\n",
    "        # END SOLUTION\n",
    "        return np.zeros((X.shape[0], 2))  # Replace with your code\n",
    "\n",
    "    def score(self, X, y):\n",
    "        # BEGIN SOLUTION\n",
    "        return np.mean(self.predict(X) == y)\n",
    "        # END SOLUTION\n",
    "        return 0.0  # Replace with your code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "n_train = 21\n",
    "d = 4\n",
    "np.random.seed(111)\n",
    "X_train_dummy = np.random.randn(n_train, d) \n",
    "y_train_dummy = np.random.randint(2, size=n_train)\n",
    "my_dc = MyDummyClassifier()\n",
    "my_dc.fit(X_train_dummy, y_train_dummy)\n",
    "my_dc.predict(X_train_dummy)\n",
    "my_dc.predict_proba(X_train_dummy)\n",
    "my_dc.score(X_train_dummy, y_train_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_dc = DummyClassifier(strategy=\"prior\") # \n",
    "sk_dc.fit(X_train_dummy, y_train_dummy)\n",
    "sk_dc.predict(X_train_dummy)\n",
    "sk_dc.predict_proba(X_train_dummy)\n",
    "sk_dc.score(X_train_dummy, y_train_dummy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are automated tests for the solution. You can run each against the student code which is expected to fail if the solution is incorrect.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "ok_format: false\n",
    "success_message: Good job!\n",
    "\"\"\" # END TEST CONFIG\n",
    "\n",
    "\n",
    "def test_it(MyDummyClassifier):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    n_train = 101\n",
    "    n_valid = 21\n",
    "    d = 5\n",
    "    for _ in range(100):\n",
    "        X_train_dummy = np.random.randn(n_train, d) \n",
    "        X_valid_dummy = np.random.randn(n_valid, d)\n",
    "        y_train_dummy = np.random.randint(2, size=n_train)\n",
    "        y_valid_dummy = np.random.randint(2, size=n_valid)\n",
    "        my_dc = MyDummyClassifier()\n",
    "        sk_dc = DummyClassifier(strategy=\"prior\")\n",
    "        _ = my_dc.fit(X_train_dummy, y_train_dummy)\n",
    "        _ = sk_dc.fit(X_train_dummy, y_train_dummy)\n",
    "        assert np.array_equal(my_dc.predict(X_train_dummy), sk_dc.predict(X_train_dummy)), \"the prediction score is wrong on a random training set\"\n",
    "        assert np.array_equal(my_dc.predict(X_valid_dummy), sk_dc.predict(X_valid_dummy)), \"the prediction score is wrong a random validation set\"\n",
    "\n",
    "test_it(MyDummyClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "ok_format: false\n",
    "success_message: Good job!\n",
    "\"\"\" # END TEST CONFIG\n",
    "\n",
    "\n",
    "def test_it(MyDummyClassifier):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    n_train = 101\n",
    "    n_valid = 21\n",
    "    d = 5\n",
    "    for _ in range(100):\n",
    "        X_train_dummy = np.random.randn(n_train, d) \n",
    "        X_valid_dummy = np.random.randn(n_valid, d)\n",
    "        y_train_dummy = np.random.randint(2, size=n_train)\n",
    "        y_valid_dummy = np.random.randint(2, size=n_valid)\n",
    "        my_dc = MyDummyClassifier()\n",
    "        sk_dc = DummyClassifier(strategy=\"prior\")\n",
    "        _ = my_dc.fit(X_train_dummy, y_train_dummy)\n",
    "        _ = sk_dc.fit(X_train_dummy, y_train_dummy)\n",
    "        assert np.allclose(my_dc.predict_proba(X_train_dummy), sk_dc.predict_proba(X_train_dummy)), \"predict_proba is wrong on a random training set\"\n",
    "        assert np.allclose(my_dc.predict_proba(X_valid_dummy), sk_dc.predict_proba(X_valid_dummy)), \"predict_proba is wrong on a random validation set\"\n",
    "\n",
    "test_it(MyDummyClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "ok_format: false\n",
    "success_message: Good job!\n",
    "\"\"\" # END TEST CONFIG\n",
    "\n",
    "\n",
    "def test_it(MyDummyClassifier):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    n_train = 101\n",
    "    n_valid = 21\n",
    "    d = 5\n",
    "    for _ in range(100):\n",
    "        X_train_dummy = np.random.randn(n_train, d) \n",
    "        X_valid_dummy = np.random.randn(n_valid, d)\n",
    "        y_train_dummy = np.random.randint(2, size=n_train)\n",
    "        y_valid_dummy = np.random.randint(2, size=n_valid)\n",
    "        my_dc = MyDummyClassifier()\n",
    "        sk_dc = DummyClassifier(strategy=\"prior\")\n",
    "        _ = my_dc.fit(X_train_dummy, y_train_dummy)\n",
    "        _ = sk_dc.fit(X_train_dummy, y_train_dummy)\n",
    "        assert np.isclose(my_dc.score(X_train_dummy, y_train_dummy), sk_dc.score(X_train_dummy, y_train_dummy)), \"the score is wrong on a random training set\"\n",
    "        assert np.isclose(my_dc.score(X_valid_dummy, y_valid_dummy), sk_dc.score(X_valid_dummy, y_valid_dummy)), \"the score is wrong on a random validation set\"\n",
    "\n",
    "test_it(MyDummyClassifier)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e3cc53df86a7e14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "## Exercise 2: Trump Tweets\n",
    "<hr>\n",
    "\n",
    "For the rest of this assignment we'll be working with a [dataset of Donald Trump's tweets](https://www.kaggle.com/austinreese/trump-tweets) as of June 2020. You should start by downloading the dataset. Unzip it and move the file `realdonaldtrump.csv` under the data directory in this folder. As usual, please do not submit the dataset when you submit the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1698308935</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701461182</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737479987</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741160716</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773561338</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         link  \\\n",
       "id                                                              \n",
       "1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                                      content  \\\n",
       "id                                                              \n",
       "1698308935  Be sure to tune in and watch Donald Trump on L...   \n",
       "1701461182  Donald Trump will be appearing on The View tom...   \n",
       "1737479987  Donald Trump reads Top Ten Financial Tips on L...   \n",
       "1741160716  New Blog Post: Celebrity Apprentice Finale and...   \n",
       "1773561338  \"My persona will never be that of a wallflower...   \n",
       "\n",
       "                           date  retweets  favorites mentions hashtags  \n",
       "id                                                                      \n",
       "1698308935  2009-05-04 13:54:25       510        917      NaN      NaN  \n",
       "1701461182  2009-05-04 20:00:10        34        267      NaN      NaN  \n",
       "1737479987  2009-05-08 08:38:08        13         19      NaN      NaN  \n",
       "1741160716  2009-05-08 15:40:15        11         26      NaN      NaN  \n",
       "1773561338  2009-05-12 09:07:28      1375       1945      NaN      NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv(\"realdonaldtrump.csv\", index_col=0)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43352, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying to predict whether a tweet will go \"viral\", defined as having more than 10,000 retweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tweets_df[\"retweets\"] > 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, we'll be using only the content (text) of the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_df[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this assignment, you can ignore all the other columns in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q2.1\n",
    "manual: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2(a) ordering the steps\n",
    "rubric={points:8}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building a model using `CountVectorizer` and `LogisticRegression`. The code required to do this has been provided below, but in the wrong order. \n",
    "\n",
    "- Rearrange the lines of code to correctly fit the model and compute the cross-validation score. \n",
    "- Add a short comment to each block to describe what the code is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# # YOUR COMMENT HERE\n",
    "# countvec = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# # YOUR COMMENT HERE\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=123)\n",
    "\n",
    "# # YOUR COMMENT HERE\n",
    "# cross_val_results = pd.DataFrame(\n",
    "#     cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "# )\n",
    "\n",
    "# # YOUR COMMENT HERE\n",
    "# pipe = make_pipeline(countvec, lr)\n",
    "\n",
    "# # YOUR COMMENT HERE\n",
    "# cross_val_results.mean()\n",
    "\n",
    "# # YOUR COMMENT HERE\n",
    "# lr = LogisticRegression(max_iter=1000, random_state=123)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time       0.620654\n",
       "score_time     0.070398\n",
       "test_score     0.895098\n",
       "train_score    0.976644\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BEGIN SOLUTION\n",
    "# 1. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=123)\n",
    "\n",
    "# 2. Create the vectorizer object\n",
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# 3. Create the log reg object (could switch with Step 2)\n",
    "lr = LogisticRegression(max_iter=1000, random_state=123)\n",
    "\n",
    "# 4. Create the Pipeline\n",
    "pipe = make_pipeline(countvec, lr)\n",
    "\n",
    "# 5. Compute the cross-validation scores across folds\n",
    "cross_val_results = pd.DataFrame(\n",
    "    \n",
    "    cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    ")\n",
    "\n",
    "# 6. Average cross-validation scores\n",
    "cross_val_results.mean()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(b) Cross-validation fold sub-scores\n",
    "rubric={points:3}\n",
    "\n",
    "Above we averaged the scores from the 5 folds of cross-validation. \n",
    "\n",
    "- Print out the 5 individual scores. \n",
    "    - (Reminder: `sklearn` calls them `\"test_score\"` but they are really (cross-)validation scores.)\n",
    "- Are the 5 scores close to each other or spread far apart? \n",
    "  - (This is a bit subjective, answer to the best of your ability.)\n",
    "- How does the size of this dataset (number of rows) compare to the cities dataset we have been using in class? How does this relate to the different sub-scores from the 5 folds?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.069167</td>\n",
       "      <td>0.891292</td>\n",
       "      <td>0.978085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.672031</td>\n",
       "      <td>0.070990</td>\n",
       "      <td>0.900231</td>\n",
       "      <td>0.976788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.606207</td>\n",
       "      <td>0.074941</td>\n",
       "      <td>0.894752</td>\n",
       "      <td>0.976644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.071249</td>\n",
       "      <td>0.889273</td>\n",
       "      <td>0.978013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.588771</td>\n",
       "      <td>0.065645</td>\n",
       "      <td>0.899942</td>\n",
       "      <td>0.973688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.672414    0.069167    0.891292     0.978085\n",
       "1  0.672031    0.070990    0.900231     0.976788\n",
       "2  0.606207    0.074941    0.894752     0.976644\n",
       "3  0.563848    0.071249    0.889273     0.978013\n",
       "4  0.588771    0.065645    0.899942     0.973688"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Are the 5 scores close to each other or spread far apart?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00495845725229569"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cross_val_results['test_score'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are pretty close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How does the size of this dataset (number of rows) compare to the cities dataset we have been using in class? How does this relate to the different sub-scores from the 5 folds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are `43352` samples in this dataset which is bigger than almost all datasets we have been exploring. Since Train set would also be bigger, the size of each fold will be bigger, and in turn, there would be less variability across folds. \n",
    "\n",
    "In more technical terms, becasue of the big size of the Train/Val sets, each fold is a better representation of the actual population, and the average performance is comparable across folds. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q2.2\n",
    "points: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2(c) baseline\n",
    "rubric={points:3}\n",
    "\n",
    "By the way, are these scores any good? \n",
    "\n",
    "- Run `DummyClassifier` (or `MyDummyClassifier`!) on this dataset.\n",
    "- Compare the `DummyClassifier` score to what you got from logistic regression above. Does logistic regression seem to be doing anything useful?\n",
    "- Is it necessary to use `CountVectorizer` here? Briefly explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cross-validation scores for the `DummyClassifier` are around 74%, which represents the fact that about 26% of the tweets in the training set are viral and 74% non-viral. \n",
    "- The logistic regression is getting around 90% accuracy so that is clearly an improvement over 74%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.891292\n",
       "1    0.900231\n",
       "2    0.894752\n",
       "3    0.889273\n",
       "4    0.899942\n",
       "Name: test_score, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_results[\"test_score\"] # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say they are quite close together, e.g. max-min ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010957324106113053"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_results[\"test_score\"].max() - cross_val_results[\"test_score\"].min() # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is quite small. This is likely due to the fairly large dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17340"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is much bigger than the cities dataset which has only 200 rows. Generally, larger datasets will give us more reliable validation results because the randomness of the splits plays less of a role."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dummy_cv_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdummy_cv_score\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAre you using the provided variable?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sha1(\u001b[38;5;28mstr\u001b[39m(np\u001b[38;5;241m.\u001b[39mround(dummy_cv_score, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mhexdigest() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m838befe9ffa0a0d530805ba36c2e4890d4148ba1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour mean CV score doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt look correct.\u001b[39m\u001b[38;5;124m\"\u001b[39m \n",
      "\u001b[0;31mNameError\u001b[0m: name 'dummy_cv_score' is not defined"
     ]
    }
   ],
   "source": [
    "assert not dummy_cv_score is None, 'Are you using the provided variable?'\n",
    "assert sha1(str(np.round(dummy_cv_score, 3)).encode('utf-8')).hexdigest() == '838befe9ffa0a0d530805ba36c2e4890d4148ba1', \"Your mean CV score doesn't look correct.\" "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q2.3\n",
    "points: 4\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q2.4\n",
    "points: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ba1f8ea22638cf75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### 2(d) probability scores\n",
    "rubric={points:5}\n",
    "\n",
    "Here we train a logistic regression classifier on the entire training set: \n",
    "\n",
    "(Note: this is relying on the `pipe` variable from 2(a) - you'll need to redefine it if you overwrote that variable in between.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your tasks:**\n",
    "\n",
    "1. Using this model, find the tweet in the **test set** with the highest predicted probability of being viral. Store the tweet and the associated probability in the variables `tweet` and `prob`, respectively. \n",
    "\n",
    "> Reminder: you are free to reuse/adapt code from lecture. Please add in a small attribution, e.g. \"From Lecture 7\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = None\n",
    "prob = None\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "most_positive_ind = np.argmax(pipe.predict_proba(X_test)[:, 1])\n",
    "prob = np.max(pipe.predict_proba(X_test)[:, 1])\n",
    "tweet = X_test.iloc[most_positive_ind]\n",
    "tweet\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not tweet is None, \"Are you using the correct variable to store the tweet?\"\n",
    "assert not prob is None, \"Are you using the correct variable to store the probability?\"\n",
    "assert sha1(str(np.round(prob, 4)).encode('utf-8')).hexdigest() == 'e8dc057d3346e56aed7cf252185dbe1fa6454411', \"Incorrect probability.\"\n",
    "assert sha1(str(tweet).encode('utf8')).hexdigest() == 'fe39d5cbae2b335b4fde5486a8df189e41392043', \"Incorrect tweet text.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q2.5\n",
    "points: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f910e9d1d6d09182",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2(e) coefficients\n",
    "rubric={points:4}\n",
    "\n",
    "We can extract the `CountVectorizer` and `LogisticRegression` objects from the `make_pipeline` object as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_from_pipe = pipe.named_steps[\"countvectorizer\"]\n",
    "lr_from_pipe = pipe.named_steps[\"logisticregression\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your tasks:**\n",
    "\n",
    "Using these extracted components above, get the five words with the highest coefficients and 5 words with smallest coefficients. Store them as lists in `top_5_words` and `bottom_5_words` variables, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_words = None\n",
    "bottom_5_words = None\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "words_weights_df = pd.DataFrame(\n",
    "    data=lr_from_pipe.coef_.ravel(),\n",
    "    index=vec_from_pipe.get_feature_names_out(),\n",
    "    columns=[\"Coefficient\"],\n",
    ")\n",
    "sorted_words_weights_df = words_weights_df.sort_values(by=\"Coefficient\", ascending=False)\n",
    "top_5_words = sorted_words_weights_df.index[0:5].tolist()\n",
    "bottom_5_words = sorted_words_weights_df.index[-5:].tolist()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not top_5_words is None, \"Are you using the correct variable?\"\n",
    "assert not bottom_5_words is None, \"Are you using the correct variable?\"\n",
    "assert len(top_5_words) == 5, \"Are you getting the top 5 words?\"\n",
    "assert len(bottom_5_words) == 5, \"Are you getting the bottom 5 words?\"\n",
    "assert sha1(\"\".join(sorted(top_5_words)).encode('utf-8')).hexdigest() == '439a61ad61c34c9069de01e2daa3a5bee63ee273', 'incorrect top 5 words'\n",
    "assert sha1(\"\".join(sorted(bottom_5_words)).encode('utf-8')).hexdigest() == '5e2af1c788307cca183334510cf6bbee0208cab2', 'incorrect bottom 5 words'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q2.6\n",
    "points: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2(f) Running a cross-validation fold without sklearn tools \n",
    "rubric={points:8}\n",
    "\n",
    "Sklearn provides a lot of useful tools like `make_pipeline` and `cross_validate`, which are awesome. But with these fancy tools it's also easy to lose track of what is actually happening under the hood. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Compute logistic regression's validation score on the first fold, that is, train on 80% and validate on 20% of the training data (`X_train`) without using sklearn `Pipeline` or `cross_validate` or `cross_val_score`. Store the score of the fold in a variable called `fold_score`. Recall that `cross_validation` in `sklearn` does not shuffle the data by default.    \n",
    "\n",
    "You should start with the following `CountVectorizer` and `LogisticRegression` objects, as well as `X_train` and `y_train` (which you should further split with `train_test_split` and `shuffle=False`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Meta-comment: you might be wondering why we're going into \"implementation\" here if this course is about _applied_ ML. In CPSC 340, we would go all the way down into `LogisticRegression` and understand how `fit` works, line by line. Here we're not going into that at all, but I still think this type of question (and Exercise 1) is a useful middle ground. I do want you to know what is going on in `Pipeline` and in `cross_validate` even if we don't cover the details of `fit`. To get into logistic regression's `fit` requires a bunch of math; here, we're keeping it more conceptual and avoiding all those prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.6\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_score = None\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "X_train_fold, X_valid_fold, y_train_fold, y_valid_fold = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, shuffle = False \n",
    ")\n",
    "\n",
    "X_train_fold_vec = countvec.fit_transform(X_train_fold)\n",
    "X_valid_fold_vec = countvec.transform(X_valid_fold)\n",
    "\n",
    "lr.fit(X_train_fold_vec, y_train_fold)\n",
    "fold_score = lr.score(X_valid_fold_vec, y_valid_fold)\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not fold_score is None, \"Are you using the correct variable?\"\n",
    "assert sha1(str(np.round(fold_score, 4)).encode('utf8')).hexdigest() == 'd267e2ea9b801d9f7ac454714a7fe6320075c255', \"The fold_score doesn't look right.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Exercise 3: Hyperparameter optimization\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q3.1\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e9e6fdea209d872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.1 Optimizing `max_features` of `CountVectorizer`\n",
    "rubric={points:4}\n",
    "\n",
    "The following code varies the `max_features` hyperparameter of `CountVectorizer` and makes a plot (with the x-axis on a log scale) that shows train/cross-validation scores vs. `max_features`. It also prints the results. \n",
    "\n",
    "**Your tasks:**\n",
    "- Based on the plot/output, what value of `max_features` seems best? Briefly explain.\n",
    "\n",
    "> The code may take a minute or two to run. You can uncomment the `print` statement if you want to see it show the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "max_features = [10, 100, 1000, 10_000, 100_000]\n",
    "\n",
    "for mf in max_features:\n",
    "    #     print(mf)    \n",
    "    pipe = make_pipeline(CountVectorizer(stop_words=\"english\", max_features=mf), LogisticRegression(max_iter=1000))\n",
    "    cv_results = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "    train_scores.append(cv_results[\"train_score\"].mean())\n",
    "    cv_scores.append(cv_results[\"test_score\"].mean())\n",
    "\n",
    "plt.semilogx(max_features, train_scores, label=\"train\")\n",
    "plt.semilogx(max_features, cv_scores, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"max_features\": max_features, \"train\": train_scores, \"cv\": cv_scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of cross-validation score, it looks like the best is `max_features=100_000`. In this case that means using all the words, since the total number of words is less than 100,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CountVectorizer(stop_words=\"english\").fit(X_train, y_train).get_feature_names_out()) # SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3adc398fdacf2efa",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q3.2\n",
    "manual: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Optimizing `C` of `LogisticRegression`\n",
    "rubric={points:6}\n",
    "\n",
    "The following code varies the `C` hyperparameter of `LogisticRegression` and makes a plot (with the x-axis on a log scale) that shows train/cross-validation scores vs. `C`. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Based on the plot, what value of `C` seems best? Briefly explain. \n",
    "\n",
    "> The code may take a minute or two to run. You can uncomment the `print` statement if you want to see it show the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "C_vals = 10.0 ** np.arange(-1.5, 2, 0.5)\n",
    "\n",
    "for C in C_vals:\n",
    "    #     print(C)\n",
    "    pipe = make_pipeline(CountVectorizer(stop_words=\"english\"), LogisticRegression(max_iter=1000, C=C))    \n",
    "    cv_results = cross_validate(pipe, X_train, y_train, return_train_score=True)\n",
    "\n",
    "    train_scores.append(cv_results[\"train_score\"].mean())\n",
    "    cv_scores.append(cv_results[\"test_score\"].mean())\n",
    "\n",
    "plt.semilogx(C_vals, train_scores, label=\"train\")\n",
    "plt.semilogx(C_vals, cv_scores, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"C\": C_vals, \"train\": train_scores, \"cv\": cv_scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.2\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the best value of `C` is 1, as it gives us the best cross-validation scores. That said, $C\\approx 0.3$ gives a very similar cross-validation score."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3adc398fdacf2efa",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q3.3\n",
    "points: \n",
    "    - 5\n",
    "    - 1\n",
    "    - 1\n",
    "    - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Hyperparameter optimization \n",
    "rubric={points:10}\n",
    "\n",
    "Start with the pipeline `pipe` below.\n",
    "\n",
    "**Your tasks:**\n",
    "- Create a `GridSearchCV` object named `grid_search` to jointly optimize `max_features` of `CountVectorizer` and `C` of `LogisticRegression` across all the combinations of values we tried above. \n",
    "- What are the best values of `max_features` and `C` according to your grid search? \n",
    "- Store them in variables `best_max_features` and `best_C`, respectively.  \n",
    "- Store the best score returned by the grid search in a variable called `best_score`. \n",
    "\n",
    "> The code might be a bit slow here. Setting `n_jobs=-1` should speed it up if you have a multi-core processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(CountVectorizer(stop_words=\"english\"), LogisticRegression(max_iter=1000, random_state=123))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid_search = None \n",
    "best_max_features = None\n",
    "best_C = None\n",
    "best_score = None\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "param_grid = {\n",
    "    \"countvectorizer__max_features\": max_features,\n",
    "    \"logisticregression__C\": C_vals,\n",
    "}\n",
    "grid_search = GridSearchCV(pipe, param_grid, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train);\n",
    "best_max_features = grid_search.best_params_['countvectorizer__max_features']\n",
    "best_C = grid_search.best_params_['logisticregression__C']\n",
    "best_score = grid_search.best_score_\n",
    "best_score\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3adc398fdacf2efa",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not best_max_features is None, \"Are you using the correct variable?\"\n",
    "assert not best_C is None, \"Are you using the correct variable?\"\n",
    "assert type(grid_search.get_params()['estimator']) == Pipeline, \"Are you passing a pipeline to the GridSearch?\"\n",
    "assert len(grid_search.get_params()['param_grid']['countvectorizer__max_features']) == 5, \"Are you using the max_features values from 3.1?\"\n",
    "assert len(grid_search.get_params()['param_grid']['logisticregression__C']) == 7, \"Are you using the C values from 3.2?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sha1(str(best_max_features).encode('utf-8')).hexdigest() == '8a12a315082a345f1a9d3ad14b214cd36d310cf8', \"Best max feature doesn't seem correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sha1(str(best_C).encode('utf-8')).hexdigest() == '98507b2da2f54e9a46bad9a285b92199e8e04200', \"Best C doesn't seem correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sha1(str(np.round(best_score,4)).encode('utf-8')).hexdigest() == '567b99788b50aef4a1ebe4338c49910c4b5363fc', \"Best score doesn't seem correct.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q3.4\n",
    "manual: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 Discussion \n",
    "rubric={points:4}\n",
    "\n",
    "- Do the best values of hyperparameters found by Grid Search agree with what you found in 3.1 and 3.2? \n",
    "- Generally speaking, _should_ these values agree with what you found in parts  3.1 and 3.2? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They don't agree. In general there is no reason they need to agree - by jointly optimizing the hyperparameters you might find something better. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3adc398fdacf2efa",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q3.5\n",
    "points: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3(e) Test score\n",
    "rubric={points:2}\n",
    "\n",
    "- Evaluate your final model on the test set. Store the test accuracy in the variable called `test_score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.5\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = None\n",
    "\n",
    "# BEGIN SOLUTION\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "test_score\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not test_score is None, \"Are you storing the score in the provided variable?\"\n",
    "assert sha1(str(np.round(test_score, 4)).encode('utf-8')).hexdigest() == 'dad0d8c6709453d08f527227df265bb497d5067e', \"The test score doesn't look correct.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q3.6\n",
    "manual: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.6 Discussion of Test Score\n",
    "rubric={points:4}\n",
    "\n",
    "- How does your test accuracy compare to your validation accuracy? \n",
    "- If they are different: do you think this is because you \"overfitted on the validation set\", or simply random luck?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.6\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test score is very close to the cross-validation score; which means our validation score was a good estimation of our test score and we have not \"overfitted\" on validation set during our HParam optimization."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q4\n",
    "manual: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Very short answer questions\n",
    "<hr>\n",
    "rubric={points:8}\n",
    "\n",
    "Each question is worth 2 points. Max 2 sentences per answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the problem with calling `fit_transform` on your test data with `CountVectorizer`? \n",
    "2. If you could only access one of `predict` or `predict_proba`, which one would you choose? Briefly explain.\n",
    "3. What are two advantages of `RandomizedSearchCV` over `GridSearchCV`?\n",
    "4. Why is it important to follow the Golden Rule? If you violate it, will that give you a worse classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You need to perform the same transformations on the train and test data, otherwise the results will not make sense.\n",
    "2. `predict_proba`. From here you can get the output of `predict`, but not the other way around.\n",
    "3. (1) you can directly choose the number of experiments, (2) avoids the \"irrelevant hyperparameter\" issue where experiments are wasted (see lecture). \n",
    "4. Violating the rule would result in overfitting the model on the test data and over-estimating the performance of the model. But, when the model is finally deployed, its performance would be lower than expected."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing the homework! \n",
    "\n",
    "![](img/eva-well-done.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
